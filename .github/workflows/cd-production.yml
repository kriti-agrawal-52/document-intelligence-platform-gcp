name: CD - Production Deployment

on:
  push:
    branches: [ main ]
  workflow_dispatch:
    inputs:
      environment:
        description: 'Deployment environment'
        required: true
        default: 'production'
        type: choice
        options:
        - production
        - staging

env:
  AWS_REGION: ap-south-1
  ECR_REGISTRY: ${{ secrets.AWS_ACCOUNT_ID }}.dkr.ecr.ap-south-1.amazonaws.com
  CLUSTER_NAME: doc-intel-cluster

jobs:
  terraform-infrastructure:
    runs-on: ubuntu-latest
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    outputs:
      ecr_auth_url: ${{ steps.terraform.outputs.auth_service_ecr_url }}
      ecr_extraction_url: ${{ steps.terraform.outputs.text_extraction_service_ecr_url }}
      ecr_summarization_url: ${{ steps.terraform.outputs.text_summarization_service_ecr_url }}
      ecr_frontend_url: ${{ steps.terraform.outputs.frontend_ecr_repository_url }}
      cluster_name: ${{ steps.terraform.outputs.eks_cluster_name }}
      app_runner_service_arn: ${{ steps.terraform.outputs.frontend_app_runner_arn }}
      
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Setup Terraform
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: ~1.5
        terraform_wrapper: false

    - name: Terraform Init
      run: |
        cd terraform
        terraform init

    - name: Terraform Validate
      run: |
        cd terraform
        echo "üîç Validating Terraform configuration..."
        terraform validate
        echo "‚úÖ Terraform configuration is valid"

    - name: Terraform Plan
      env:
        TF_LOG: DEBUG
      run: |
        cd terraform
        
        # Check if all required variables are set
        echo "üîç Checking environment variables..."
        echo "AWS_REGION: ${{ env.AWS_REGION }}"
        echo "Environment: ${{ github.event.inputs.environment || 'production' }}"
        echo "OpenAI API Key: $(if [ -n "${{ secrets.OPENAI_API_KEY }}" ]; then echo "‚úÖ Set"; else echo "‚ùå Missing"; fi)"
        echo "JWT Secret: $(if [ -n "${{ secrets.JWT_SECRET_KEY }}" ]; then echo "‚úÖ Set"; else echo "‚ùå Missing"; fi)"
        
        # Run terraform plan and capture exit code
        set +e  # Don't exit on error immediately
        echo "üöÄ Running Terraform plan..."
        terraform plan -var="aws_region=${{ env.AWS_REGION }}" \
                      -var="environment=${{ github.event.inputs.environment || 'production' }}" \
                      -var="openai_api_key=${{ secrets.OPENAI_API_KEY }}" \
                      -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" \
                      -out=tfplan
        PLAN_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        
        # Check terraform plan exit codes
        # 0 = No changes, 2 = Changes present, 1 = Error
        if [ $PLAN_EXIT_CODE -eq 0 ]; then
          echo "‚úÖ Terraform plan completed - No changes needed"
        elif [ $PLAN_EXIT_CODE -eq 2 ]; then
          echo "‚úÖ Terraform plan completed - Changes detected (this is expected)"
        else
          echo "‚ùå Terraform plan failed with exit code $PLAN_EXIT_CODE"
          echo "üîç Running terraform plan again to show detailed errors..."
          
          # Run plan again without -out flag to show full error details
          terraform plan -var="aws_region=${{ env.AWS_REGION }}" \
                        -var="environment=${{ github.event.inputs.environment || 'production' }}" \
                        -var="openai_api_key=${{ secrets.OPENAI_API_KEY }}" \
                        -var="jwt_secret_key=${{ secrets.JWT_SECRET_KEY }}" || true
          
          echo "‚ùå Plan generation failed - cannot proceed to apply"
          exit 1
        fi
        
        # Verify plan file was created
        if [ ! -f tfplan ]; then
          echo "‚ùå Terraform plan file was not created"
          exit 1
        fi
        
        echo "‚úÖ Terraform plan file created successfully"

    - name: Pre-Apply AWS Quota Check
      run: |
        echo "üîç Checking AWS Service Quotas before deployment..."
        
        # Check EC2 running instances
        RUNNING_INSTANCES=$(aws ec2 describe-instances --region ${{ env.AWS_REGION }} \
          --filters "Name=instance-state-name,Values=running,pending" \
          --query 'length(Reservations[].Instances[])')
        echo "Current running/pending EC2 instances: $RUNNING_INSTANCES"
        
        # Check Spot Fleet Requests
        SPOT_FLEETS=$(aws ec2 describe-spot-fleet-requests --region ${{ env.AWS_REGION }} \
          --query 'length(SpotFleetRequestConfigs[?SpotFleetRequestState==`active`])')
        echo "Active Spot Fleet Requests: $SPOT_FLEETS"
        
        # Check existing failed resources that might need cleanup
        echo "üîç Checking for existing failed resources..."
        
        # Check for failed App Runner services
        FAILED_APPRUNNER=$(aws apprunner list-services --region ${{ env.AWS_REGION }} \
          --query 'length(ServiceSummaryList[?contains(ServiceName,`document-intelligence`)])' 2>/dev/null || echo "0")
        if [ "$FAILED_APPRUNNER" -gt 0 ]; then
          echo "‚ö†Ô∏è  Found $FAILED_APPRUNNER existing App Runner services - cleanup may be needed"
        fi
        
        # Check for existing subnet groups
        RDS_SUBNETS=$(aws rds describe-db-subnet-groups --region ${{ env.AWS_REGION }} \
          --query 'length(DBSubnetGroups[?contains(DBSubnetGroupName,`rds-subnet-group`)])' 2>/dev/null || echo "0")
        CACHE_SUBNETS=$(aws elasticache describe-cache-subnet-groups --region ${{ env.AWS_REGION }} \
          --query 'length(CacheSubnetGroups[?contains(CacheSubnetGroupName,`redis-subnet-group`)])' 2>/dev/null || echo "0")
        
        if [ "$RDS_SUBNETS" -gt 0 ] || [ "$CACHE_SUBNETS" -gt 0 ]; then
          echo "‚ö†Ô∏è  Found existing subnet groups (RDS: $RDS_SUBNETS, Cache: $CACHE_SUBNETS)"
          echo "üí° Consider running cleanup script: ./aws-cleanup-complete.sh"
        fi

    - name: Terraform Apply
      id: terraform
      run: |
        cd terraform
        echo "üöÄ Starting Terraform Apply..."
        
        # Apply the plan with detailed error capture
        set +e  # Don't exit on error immediately
        terraform apply -auto-approve tfplan 2>&1 | tee terraform_apply.log
        APPLY_EXIT_CODE=$?
        set -e  # Re-enable exit on error
        
        if [ $APPLY_EXIT_CODE -eq 0 ]; then
          echo "‚úÖ Terraform apply completed successfully"
        else
          echo "‚ùå Terraform apply failed with exit code $APPLY_EXIT_CODE"
          echo ""
          echo "üîç Analyzing failure reasons..."
          
          # Check for common error patterns
          if grep -q "DBSubnetGroupAlreadyExists\|CacheSubnetGroupAlreadyExists" terraform_apply.log; then
            echo "üí° SOLUTION: Subnet group already exists. Run cleanup script:"
            echo "   ./aws-cleanup-complete.sh"
          fi
          
          if grep -q "CREATE_FAILED.*App Runner" terraform_apply.log; then
            echo "üí° SOLUTION: App Runner service failed. Check service logs and run cleanup:"
            echo "   ./aws-cleanup-complete.sh"
          fi
          
          if grep -q "AsgInstanceLaunchFailures\|Fleet Requests.*quota" terraform_apply.log; then
            echo "üí° SOLUTION: EC2 quota exceeded. Options:"
            echo "   1. Run cleanup script: ./aws-cleanup-complete.sh"
            echo "   2. Request quota increase in AWS Service Quotas console"
            echo "   3. Wait for existing instances to terminate"
          fi
          
          if grep -q "ResourceAlreadyExistsException\|AlreadyExists" terraform_apply.log; then
            echo "üí° SOLUTION: Resources already exist. Run cleanup script:"
            echo "   ./aws-cleanup-complete.sh"
          fi
          
          echo ""
          echo "üìã Troubleshooting Steps:"
          echo "1. Run the enhanced cleanup script: ./aws-cleanup-complete.sh"
          echo "2. Wait 15-20 minutes for resources to be fully deleted"
          echo "3. Check AWS Service Quotas console for limits"
          echo "4. Re-run this deployment"
          
          exit 1
        fi
        
        echo "üìä Exporting Terraform outputs..."
        
        # Debug: Show all terraform outputs
        echo "üîç All Terraform outputs:"
        terraform output
        
        # Export outputs for next jobs (with error handling and validation)
        AUTH_ECR_URL=$(terraform output -raw auth_service_ecr_url 2>/dev/null || echo "")
        EXTRACTION_ECR_URL=$(terraform output -raw text_extraction_service_ecr_url 2>/dev/null || echo "")
        SUMMARIZATION_ECR_URL=$(terraform output -raw text_summarization_service_ecr_url 2>/dev/null || echo "")
        
        echo "üîç ECR URLs:"
        echo "  Auth: $AUTH_ECR_URL"
        echo "  Extraction: $EXTRACTION_ECR_URL"
        echo "  Summarization: $SUMMARIZATION_ECR_URL"
        
        # Validate ECR URLs are not empty
        if [ -z "$AUTH_ECR_URL" ] || [ -z "$EXTRACTION_ECR_URL" ] || [ -z "$SUMMARIZATION_ECR_URL" ]; then
          echo "‚ùå ERROR: One or more ECR URLs are empty!"
          echo "This indicates that ECR repositories were not created successfully."
          echo "Please check the Terraform apply logs for ECR creation errors."
          exit 1
        fi
        
        echo "auth_service_ecr_url=$AUTH_ECR_URL" >> $GITHUB_OUTPUT
        echo "text_extraction_service_ecr_url=$EXTRACTION_ECR_URL" >> $GITHUB_OUTPUT
        echo "text_summarization_service_ecr_url=$SUMMARIZATION_ECR_URL" >> $GITHUB_OUTPUT
        echo "eks_cluster_name=$(terraform output -raw eks_cluster_name)" >> $GITHUB_OUTPUT
        echo "rds_endpoint=$(terraform output -raw rds_endpoint)" >> $GITHUB_OUTPUT
        echo "docdb_endpoint=$(terraform output -raw docdb_endpoint)" >> $GITHUB_OUTPUT
        echo "redis_endpoint=$(terraform output -raw redis_endpoint)" >> $GITHUB_OUTPUT
        echo "sqs_queue_url=$(terraform output -raw sqs_summarization_queue_url)" >> $GITHUB_OUTPUT
        echo "s3_bucket_name=$(terraform output -raw user_images_bucket_name)" >> $GITHUB_OUTPUT
        
        # Frontend outputs (may not exist if frontend resources fail to create)
        if terraform output -raw frontend_ecr_repository_url >/dev/null 2>&1; then
          echo "frontend_ecr_repository_url=$(terraform output -raw frontend_ecr_repository_url)" >> $GITHUB_OUTPUT
          echo "‚úÖ Frontend ECR repository URL exported"
        else
          echo "frontend_ecr_repository_url=" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è  Frontend ECR repository URL not available"
        fi
        
        if terraform output -raw frontend_app_runner_arn >/dev/null 2>&1; then
          echo "frontend_app_runner_arn=$(terraform output -raw frontend_app_runner_arn)" >> $GITHUB_OUTPUT
          echo "‚úÖ Frontend App Runner ARN exported"
        else
          echo "frontend_app_runner_arn=" >> $GITHUB_OUTPUT
          echo "‚ö†Ô∏è  Frontend App Runner ARN not available"
        fi
        
        echo "‚úÖ All outputs exported successfully"

    - name: Post-Apply Resource Health Check
      if: always()
      run: |
        echo "üè• Checking resource health after Terraform apply..."
        
        # Check App Runner service status
        if [ -n "${{ steps.terraform.outputs.frontend_app_runner_arn }}" ]; then
          echo "Checking App Runner service status..."
          SERVICE_ARN="${{ steps.terraform.outputs.frontend_app_runner_arn }}"
          SERVICE_STATUS=$(aws apprunner describe-service --service-arn "$SERVICE_ARN" --region ${{ env.AWS_REGION }} \
            --query 'Service.Status' --output text 2>/dev/null || echo "NOT_FOUND")
          echo "App Runner service status: $SERVICE_STATUS"
          
          if [ "$SERVICE_STATUS" = "CREATE_FAILED" ]; then
            echo "‚ùå App Runner service creation failed"
            echo "üîç Getting failure details..."
            aws apprunner describe-service --service-arn "$SERVICE_ARN" --region ${{ env.AWS_REGION }} \
              --query 'Service' --output json || true
            
            echo "üí° This service will be cleaned up in the next deployment cycle"
          elif [ "$SERVICE_STATUS" = "RUNNING" ]; then
            echo "‚úÖ App Runner service is healthy"
          else
            echo "‚è≥ App Runner service is in transitional state: $SERVICE_STATUS"
          fi
        fi
        
        # Check EKS node group status
        if [ -n "${{ steps.terraform.outputs.eks_cluster_name }}" ]; then
          echo "Checking EKS node groups..."
          CLUSTER_NAME="${{ steps.terraform.outputs.eks_cluster_name }}"
          NODE_GROUPS=$(aws eks list-nodegroups --cluster-name "$CLUSTER_NAME" --region ${{ env.AWS_REGION }} \
            --query 'nodegroups' --output text 2>/dev/null || echo "")
          
          for NODE_GROUP in $NODE_GROUPS; do
            if [ "$NODE_GROUP" != "None" ] && [ ! -z "$NODE_GROUP" ]; then
              NODE_STATUS=$(aws eks describe-nodegroup --cluster-name "$CLUSTER_NAME" --nodegroup-name "$NODE_GROUP" --region ${{ env.AWS_REGION }} \
                --query 'nodegroup.status' --output text 2>/dev/null || echo "UNKNOWN")
              echo "Node group $NODE_GROUP status: $NODE_STATUS"
              
              if [ "$NODE_STATUS" = "CREATE_FAILED" ]; then
                echo "‚ùå Node group creation failed due to quota limits"
                echo "üí° Run cleanup script and consider requesting quota increase"
              fi
            fi
          done
        fi

  build-and-push-images:
    runs-on: ubuntu-latest
    needs: terraform-infrastructure
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    strategy:
      matrix:
        service: 
          - name: user_auth
            ecr_url: ${{ needs.terraform-infrastructure.outputs.ecr_auth_url }}
          - name: text_extraction
            ecr_url: ${{ needs.terraform-infrastructure.outputs.ecr_extraction_url }}
          - name: text_summarization
            ecr_url: ${{ needs.terraform-infrastructure.outputs.ecr_summarization_url }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Build, tag, and push image - ${{ matrix.service.name }}
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
      run: |
        echo "üîç Debug information:"
        echo "  Service name: ${{ matrix.service.name }}"
        echo "  ECR URL: ${{ matrix.service.ecr_url }}"
        echo "  ECR Registry: $ECR_REGISTRY"
        echo "  Image Tag: $IMAGE_TAG"
        
        # Validate ECR URL is not empty
        if [ -z "${{ matrix.service.ecr_url }}" ]; then
          echo "‚ùå ERROR: ECR URL is empty for service ${{ matrix.service.name }}"
          echo "This indicates that the Terraform output was not properly exported."
          exit 1
        fi
        
        # Build the Docker image (using standard docker build - more reliable)
        echo "üèóÔ∏è Building Docker image..."
        docker build -f ${{ matrix.service.name }}/Dockerfile -t ${{ matrix.service.ecr_url }}:$IMAGE_TAG .
        
        echo "üè∑Ô∏è Tagging Docker image..."
        docker tag ${{ matrix.service.ecr_url }}:$IMAGE_TAG ${{ matrix.service.ecr_url }}:latest
        
        echo "üì§ Pushing Docker images to ECR..."
        # Push the Docker image to ECR with retry logic
        for i in {1..3}; do
          if docker push ${{ matrix.service.ecr_url }}:$IMAGE_TAG && docker push ${{ matrix.service.ecr_url }}:latest; then
            echo "‚úÖ Successfully pushed images on attempt $i"
            break
          else
            echo "‚ö†Ô∏è Push failed on attempt $i, retrying..."
            sleep 10
          fi
        done
        
        echo "‚úÖ Successfully built and pushed ${{ matrix.service.name }} image"

  deploy-to-kubernetes:
    runs-on: ubuntu-latest
    needs: [terraform-infrastructure, build-and-push-images]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    outputs:
      alb_url: ${{ steps.get-alb-url.outputs.alb_url }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Update kubeconfig
      run: |
        aws eks update-kubeconfig --region ${{ env.AWS_REGION }} --name ${{ needs.terraform-infrastructure.outputs.cluster_name }}

    - name: Install External Secrets Operator
      run: |
        helm repo add external-secrets https://charts.external-secrets.io
        helm repo update
        
        # Install External Secrets Operator if not already installed
        helm upgrade --install external-secrets external-secrets/external-secrets \
          -n external-secrets-system \
          --create-namespace \
          --wait

    - name: Process Kubernetes manifests
      run: |
        # Create processed manifests directory
        mkdir -p kubernetes_processed
        cp -r kubernetes/* kubernetes_processed/
        
        # Replace placeholders with actual values from Terraform
        AWS_ACCOUNT_ID=${{ secrets.AWS_ACCOUNT_ID }}
        IMAGE_TAG=${{ github.sha }}
        
        # Replace ECR image URLs with versioned tags
        sed -i "s|<aws_account_id>|${AWS_ACCOUNT_ID}|g" kubernetes_processed/*.yaml
        sed -i "s|:latest|:${IMAGE_TAG}|g" kubernetes_processed/*.yaml
        
        # Replace infrastructure endpoints (these would come from Terraform outputs)
        sed -i "s|<replace-with-sqs-queue-url-from-terraform-output>|${{ needs.terraform-infrastructure.outputs.sqs_queue_url }}|g" kubernetes_processed/*.yaml
        sed -i "s|<replace-with-s3-bucket-name-from-terraform-output>|${{ needs.terraform-infrastructure.outputs.s3_bucket_name }}|g" kubernetes_processed/*.yaml
        
        # Update database service endpoints
        sed -i "s|externalName: # <--- REPLACE with your RDS endpoint|externalName: ${{ needs.terraform-infrastructure.outputs.rds_endpoint }}|g" kubernetes_processed/02-db-services.yaml
        sed -i "s|externalName: # <--- REPLACE with your DocumentDB endpoint|externalName: ${{ needs.terraform-infrastructure.outputs.docdb_endpoint }}|g" kubernetes_processed/02-db-services.yaml
        sed -i "s|externalName: # <--- REPLACE with your Redis endpoint|externalName: ${{ needs.terraform-infrastructure.outputs.redis_endpoint }}|g" kubernetes_processed/02-db-services.yaml

    - name: Deploy to Kubernetes
      run: |
        # Apply manifests in dependency order
        kubectl apply -f kubernetes_processed/00-namespace.yaml
        kubectl apply -f kubernetes_processed/01-secrets.yaml
        kubectl apply -f kubernetes_processed/02-db-services.yaml
        
        # Wait for External Secrets to sync
        echo "Waiting for secrets to sync..."
        sleep 30
        
        # Deploy applications
        kubectl apply -f kubernetes_processed/03-auth-deployment.yaml
        kubectl apply -f kubernetes_processed/04-text-extraction-deployment.yaml
        kubectl apply -f kubernetes_processed/08-text-summarization-deployment.yaml
        
        # Configure auto-scaling and load balancing
        kubectl apply -f kubernetes_processed/06-auth-hpa.yaml
        kubectl apply -f kubernetes_processed/07-text-extraction-hpa.yaml
        kubectl apply -f kubernetes_processed/05-ingress.yaml

    - name: Wait for deployment to be ready
      run: |
        # Wait for deployments to be ready
        kubectl wait --for=condition=available --timeout=300s deployment/auth-deployment -n doc-intel-app
        kubectl wait --for=condition=available --timeout=300s deployment/text-extraction-deployment -n doc-intel-app
        kubectl wait --for=condition=available --timeout=300s deployment/text-summarisation-deployment -n doc-intel-app

    - name: Get ALB URL
      id: get-alb-url
      run: |
        # Get ALB URL
        ALB_URL=$(kubectl get ingress doc-intel-ingress -n doc-intel-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        
        if [ -z "$ALB_URL" ]; then
          echo "ALB URL not available yet, waiting..."
          sleep 60
          ALB_URL=$(kubectl get ingress doc-intel-ingress -n doc-intel-app -o jsonpath='{.status.loadBalancer.ingress[0].hostname}')
        fi
        
        echo "ALB URL: $ALB_URL"
        echo "alb_url=$ALB_URL" >> $GITHUB_OUTPUT

    - name: Run deployment verification tests
      run: |
        ALB_URL="${{ steps.get-alb-url.outputs.alb_url }}"
        
        # Test health endpoints
        curl -f http://$ALB_URL/auth/health || exit 1
        curl -f http://$ALB_URL/extract/health || exit 1
        curl -f http://$ALB_URL/health || exit 1
        
        echo "All health checks passed!"

    - name: Create GitHub deployment
      uses: actions/github-script@v7
      with:
        script: |
          const deployment = await github.rest.repos.createDeployment({
            owner: context.repo.owner,
            repo: context.repo.repo,
            ref: context.sha,
            environment: '${{ github.event.inputs.environment || 'production' }}',
            description: 'Deployed via GitHub Actions'
          });
          
          await github.rest.repos.createDeploymentStatus({
            owner: context.repo.owner,
            repo: context.repo.repo,
            deployment_id: deployment.data.id,
            state: 'success',
            description: 'Deployment completed successfully'
          });

  deploy-frontend:
    runs-on: ubuntu-latest
    needs: [terraform-infrastructure, deploy-to-kubernetes]
    environment: ${{ github.event.inputs.environment || 'production' }}
    if: needs.terraform-infrastructure.outputs.frontend_ecr_repository_url != ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: frontend/package-lock.json

    - name: Setup pnpm
      uses: pnpm/action-setup@v2
      with:
        version: '8'
        run_install: false

    - name: Get pnpm store directory
      shell: bash
      run: echo "STORE_PATH=$(pnpm store path --silent)" >> $GITHUB_ENV

    - name: Setup pnpm cache
      uses: actions/cache@v3
      with:
        path: ${{ env.STORE_PATH }}
        key: ${{ runner.os }}-pnpm-store-${{ hashFiles('frontend/pnpm-lock.yaml') }}
        restore-keys: |
          ${{ runner.os }}-pnpm-store-

    - name: Install Dependencies
      working-directory: ./frontend
      run: pnpm install --frozen-lockfile

    - name: Build Application
      working-directory: ./frontend
      run: pnpm build
      env:
        # Get ALB URL for backend services
        NEXT_PUBLIC_AUTH_SERVICE_URL: http://${{ needs.deploy-to-kubernetes.outputs.alb_url }}
        NEXT_PUBLIC_EXTRACTION_SERVICE_URL: http://${{ needs.deploy-to-kubernetes.outputs.alb_url }}
        NEXT_PUBLIC_SUMMARIZATION_SERVICE_URL: http://${{ needs.deploy-to-kubernetes.outputs.alb_url }}
        NODE_ENV: production

    - name: Configure AWS credentials
      uses: aws-actions/configure-aws-credentials@v4
      with:
        aws-access-key-id: ${{ secrets.AWS_ACCESS_KEY_ID }}
        aws-secret-access-key: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        aws-region: ${{ env.AWS_REGION }}

    - name: Login to Amazon ECR
      id: login-ecr
      uses: aws-actions/amazon-ecr-login@v2

    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
      with:
        driver: docker
        install: true

    - name: Build and Push Frontend Docker Image
      timeout-minutes: 15
      env:
        ECR_REGISTRY: ${{ steps.login-ecr.outputs.registry }}
        IMAGE_TAG: ${{ github.sha }}
        FRONTEND_ECR_URL: ${{ needs.terraform-infrastructure.outputs.ecr_frontend_url }}
      run: |
        echo "üîç Frontend build debug information:"
        echo "  Frontend ECR URL: $FRONTEND_ECR_URL"
        echo "  Image Tag: $IMAGE_TAG"
        echo "  ALB URL: ${{ steps.get-alb-url.outputs.alb_url }}"
        
        # Validate ECR URL is not empty
        if [ -z "$FRONTEND_ECR_URL" ]; then
          echo "‚ùå ERROR: Frontend ECR URL is empty"
          exit 1
        fi
        
        # Build the frontend Docker image
        echo "üèóÔ∏è Building frontend Docker image..."
        docker build \
          --build-arg NODE_ENV=production \
          --build-arg NEXT_TELEMETRY_DISABLED=1 \
          --build-arg NEXT_PUBLIC_AUTH_SERVICE_URL="${{ steps.get-alb-url.outputs.alb_url }}" \
          --build-arg NEXT_PUBLIC_EXTRACTION_SERVICE_URL="${{ steps.get-alb-url.outputs.alb_url }}" \
          --build-arg NEXT_PUBLIC_SUMMARIZATION_SERVICE_URL="${{ steps.get-alb-url.outputs.alb_url }}" \
          -f frontend/Dockerfile \
          -t $FRONTEND_ECR_URL:$IMAGE_TAG \
          ./frontend
        
        echo "üè∑Ô∏è Tagging frontend image..."
        docker tag $FRONTEND_ECR_URL:$IMAGE_TAG $FRONTEND_ECR_URL:latest
        
        echo "üì§ Pushing frontend images to ECR..."
        # Push with retry logic
        for i in {1..3}; do
          if docker push $FRONTEND_ECR_URL:$IMAGE_TAG && docker push $FRONTEND_ECR_URL:latest; then
            echo "‚úÖ Successfully pushed frontend images on attempt $i"
            break
          else
            echo "‚ö†Ô∏è Frontend push failed on attempt $i, retrying..."
            sleep 10
          fi
        done
        
        echo "‚úÖ Frontend Docker image built and pushed successfully"

    - name: Wait for App Runner Deployment
      run: |
        SERVICE_ARN="${{ needs.terraform-infrastructure.outputs.app_runner_service_arn }}"
        echo "üöÄ Waiting for App Runner deployment to complete..."
        
        max_attempts=90
        attempt=1
        
        while [[ $attempt -le $max_attempts ]]; do
          status=$(aws apprunner describe-service \
            --service-arn "$SERVICE_ARN" \
            --query 'Service.Status' \
            --output text)
          
          echo "üìä Deployment status: $status (attempt $attempt/$max_attempts)"
          
          case $status in
            "RUNNING")
              echo "‚úÖ App Runner service is running successfully!"
              break
              ;;
            "OPERATION_IN_PROGRESS")
              echo "‚è≥ Deployment in progress..."
              ;;
            "CREATE_FAILED"|"UPDATE_FAILED"|"DELETE_FAILED")
              echo "‚ùå App Runner deployment failed with status: $status"
              exit 1
              ;;
          esac
          
          if [[ $attempt -eq $max_attempts ]]; then
            echo "‚è∞ Deployment monitoring timed out"
            exit 1
          fi
          
          sleep 10
          ((attempt++))
        done

    - name: Get Frontend URL and Health Check
      id: frontend-info
      run: |
        SERVICE_ARN="${{ needs.terraform-infrastructure.outputs.app_runner_service_arn }}"
        
        SERVICE_URL=$(aws apprunner describe-service \
          --service-arn "$SERVICE_ARN" \
          --query 'Service.ServiceUrl' \
          --output text)
        
        FRONTEND_URL="https://$SERVICE_URL"
        echo "frontend-url=$FRONTEND_URL" >> $GITHUB_OUTPUT
        echo "üåê Frontend URL: $FRONTEND_URL"
        
        # Health check
        HEALTH_URL="$FRONTEND_URL/api/health"
        echo "üè• Performing health check: $HEALTH_URL"
        
        max_attempts=12
        attempt=1
        
        while [[ $attempt -le $max_attempts ]]; do
          if curl -f -s "$HEALTH_URL" > /dev/null; then
            echo "‚úÖ Frontend health check passed!"
            break
          else
            echo "‚è≥ Health check failed, retrying... (attempt $attempt/$max_attempts)"
            
            if [[ $attempt -eq $max_attempts ]]; then
              echo "‚ùå Frontend health check failed"
              exit 1
            fi
          fi
          
          sleep 10
          ((attempt++))
        done

  post-deployment-tests:
    runs-on: ubuntu-latest
    needs: [deploy-to-kubernetes, deploy-frontend]
    environment: ${{ github.event.inputs.environment || 'production' }}
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Run end-to-end tests
      run: |
        pip install pytest requests
        # Run E2E tests against the deployed application
        python -m pytest tests/e2e/ -v --tb=short
      env:
        API_BASE_URL: ${{ needs.deploy-to-kubernetes.outputs.alb_url }}
      continue-on-error: true

    - name: Notify deployment status
      uses: actions/github-script@v7
      if: always()
      with:
        script: |
          const status = '${{ job.status }}' === 'success' ? 'success' : 'failure';
          const backendUrl = '${{ needs.deploy-to-kubernetes.outputs.alb_url }}';
          const frontendUrl = '${{ needs.deploy-frontend.outputs.frontend-url }}' || 'Not available';
          
          const message = status === 'success' 
            ? `üöÄ Production deployment completed successfully!
            
            **Backend Services:** http://${backendUrl}
            **Frontend Application:** ${frontendUrl}
            
            **Health Checks:**
            - Auth Service: http://${backendUrl}/auth/health
            - Extraction Service: http://${backendUrl}/extract/health  
            - Frontend Health: ${frontendUrl}/api/health
            ` 
            : '‚ùå Production deployment failed. Please check the logs.';
          
          if (context.issue && context.issue.number) {
            github.rest.issues.createComment({
              issue_number: context.issue.number,
              owner: context.repo.owner,
              repo: context.repo.repo,
              body: message
            });
          } else {
            console.log('No issue/PR context, skipping comment');
            console.log(message);
          }
